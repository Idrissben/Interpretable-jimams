# Algorithmic Fairness and Interpretability - JIMAMS
This repository contains code for building a model that predicts whether an individual has been granted parole or not using a dataset that contains different features (sex, crimes, ethnicity, age...), and then dives deep into assessing the model's fairness and interpretability.

## Team JIMAMS
This project was developed bu team JIMAMS during the 'Algorithmic Fairness and Interpretability' course. The team members are:

- Idriss Bennis
- Ajouad Akjouj
- Mathieu Péharpré
- Joseph Moussa
- Samuel Berrebi
- Marie-Sophie Richard

## Project Steps

- Step 1: Dataset description and exploratory data analysis
- Step 2: XGBoost model training
- Step 3: Comparison with white box models
- Step 4: Surrogate method for XGBoost interpretation
- Step 5: PDP and ALE for XGBoost interpretation
- Step 6: Post-hoc local methods for local interpretability
- Step 7: Performance interpretability using permutation importance and XPER
- Step 8: Model fairness with respect to protected attributes and mitigation
- Step 9: FPDP implementation using a fairness measure